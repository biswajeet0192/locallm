# ğŸ§  Local LLM Chatbot â€“ Backend (FastAPI)

This is the **FastAPI backend** for the Local LLM Chatbot application.  
It provides API endpoints to interact with the local LLM (via Ollama) and serves data to the React frontend.

---

## ğŸ“ Project Structure (Backend)

```
backend/
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ chat.py            # Routes for chat requests
â”œâ”€â”€ services/
â”‚   â””â”€â”€ ollama_service.py  # Logic to communicate with Ollama
â”œâ”€â”€ main.py                # FastAPI app entry point
â””â”€â”€ requirements.txt       # Python dependencies
```

---

## ğŸš€ Features (Backend)

- âœ… FastAPI backend to handle chat and model queries  
- âœ… Start/stop Ollama server from API  
- âœ… List and switch between installed models  
- âœ… Generate model responses via API  
- âœ… 100% local â€“ private by design  

---

## ğŸ› ï¸ Requirements

- [Python 3.9+](https://www.python.org/downloads/)  
- [Ollama](https://ollama.com/download) installed locally  

---

## âš™ï¸ Setup & Run (Backend)

```bash
cd backend
python3 -m venv venv
source venv/bin/activate     # Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

Backend will run at: **http://localhost:8000**

---

## ğŸ“¡ API Endpoints

| Method | Endpoint   | Description                    |
|--------|------------|--------------------------------|
| GET    | `/models`  | Get a list of installed models |
| POST   | `/generate`| Generate a response from model |
| GET    | `/status`  | Check if Ollama server running |
| POST   | `/start`   | Start the Ollama server        |

---

## ğŸ› ï¸ Tech Stack

- **FastAPI** (Python web framework)  
- **Uvicorn** (ASGI server)  
- **Ollama** (local LLM runtime)  

---

## ğŸ¤ Contributing

Pull requests are welcome! For major changes, please open an issue first to discuss your ideas.
