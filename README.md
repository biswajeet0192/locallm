# ğŸ§  Local LLM Chatbot with Ollama, FastAPI & React

A full-stack chatbot application that runs a **local LLM (Llama 3.x or any Ollama model)** with a **FastAPI backend** and a **React frontend**.  

It allows you to:

- **Start/stop the Ollama server**
- **View installed models**
- **Select a model for inference**
- **Chat in a ChatGPT-style interface â€” all locally**

---

## ğŸ“ Project Structure

```
ollama-chatbot/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatBox.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ModelSelector.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ServerStatus.jsx
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useChat.js
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js
â”‚   â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”‚   â””â”€â”€ index.js
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â””â”€â”€ index.js
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.js
â””â”€â”€ backend/
    â”œâ”€â”€ routes/
    â”‚   â””â”€â”€ chat.py
    â”œâ”€â”€ services/
    â”‚   â””â”€â”€ ollama_service.py
    â”œâ”€â”€ main.py
    â””â”€â”€ requirements.txt
```

---

## ğŸš€ Features

- âœ… Local LLM inference using [Ollama](https://ollama.com)  
- âœ… FastAPI backend to handle chat and model queries  
- âœ… React frontend with a modern ChatGPT-style UI  
- âœ… Model management: list, select, and switch between installed models  
- âœ… Start/stop Ollama server directly from the UI  
- âœ… Lightweight and 100% local â€” private by design  

---

## ğŸ› ï¸ Requirements

- [Python 3.9+](https://www.python.org/downloads/)  
- [Node.js 18+](https://nodejs.org/)  
- [Ollama](https://ollama.com/download) installed locally  

---

## âš™ï¸ Backend Setup (FastAPI)

```bash
cd backend
python3 -m venv venv
source venv/bin/activate     # Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

Backend will run at: **http://localhost:8000**

---

## ğŸ§  Using the App

1. Ensure **Ollama** is installed and at least one model is downloaded:

   ```bash
   ollama pull llama3
   ```

2. Start the backend (FastAPI with Uvicorn):

   ```bash
   uvicorn main:app --reload
   ```

3. Start the frontend:

   ```bash
   npm run dev
   ```

4. Open **http://localhost:3000** in your browser.

   - ğŸ”˜ Click **â€œStart Ollamaâ€** if the server isnâ€™t running  
   - ğŸ“œ Click **â€œGet Modelsâ€** to list installed models  
   - ğŸ¤– Select a model and start chatting  

---

## ğŸ“¡ API Endpoints

| Method | Endpoint   | Description                    |
|--------|------------|--------------------------------|
| GET    | `/models`  | Get a list of installed models |
| POST   | `/generate`| Generate a response from model |
| GET    | `/status`  | Check if Ollama server running |
| POST   | `/start`   | Start the Ollama server        |

---

## ğŸ› ï¸ Tech Stack

- **Frontend:** React, TailwindCSS, Lucide Icons  
- **Backend:** FastAPI, Python  
- **LLM Runtime:** Ollama (Llama 3.x, Mistral, etc.)  

---

## ğŸ¤ Contributing

Pull requests are welcome! For major changes, please open an issue first to discuss your ideas.
